{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rockpool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from snntorch import spikeplot as splt\n",
    "from rockpool.nn.modules import LinearTorch, LIFTorch\n",
    "from rockpool.nn.combinators import Sequential\n",
    "from rockpool.parameters import Constant\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download del datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "data_root='../data'\n",
    "# Device and data repository\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trasformazioni da applicare al Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.,), (1,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "mnist_train = datasets.MNIST(data_root, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_root, train=False, download=True, transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione della dimensione del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione dei dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def plot_mem_rec(mem_rec, batch_size, targets):\n",
    "    num_steps = len(mem_rec)\n",
    "    for i in range(0,batch_size):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.subplots()\n",
    "        ax.set_xlim((-10,210))\n",
    "        ax.set_ylim((-2,2))\n",
    "        ss = mem_rec[:,i,:]\n",
    "        plt.plot(range(0,num_steps), ss.cpu().detach())\n",
    "\n",
    "        plt.title(\"Output Layer Membrane Output - {}\".format(targets[i]))\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Neuron Number\")\n",
    "        fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_spk_rec(spk_rec, batch_size, targets):\n",
    "    for i in range(0,batch_size):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.subplots()\n",
    "        ax.set_xlim((-10,210))\n",
    "        ax.set_ylim((-1,11))\n",
    "        ax.set_yticks(range(0,11))\n",
    "        ss = spk_rec[i,:,:]\n",
    "        splt.raster(ss, ax, s=1, c=\"black\")\n",
    "\n",
    "        plt.title(\"Output Layer - {}\".format(targets[i]))\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Neuron Number\")\n",
    "        fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_batch_accuracy(net, data, targets, train=False):\n",
    "    spk_rec, _, _ = net(data)\n",
    "    _, data_idx = spk_rec.sum(dim=(1)).max(1)\n",
    "    _, targets_idx = targets.sum(dim=(1)).max(1)\n",
    "    acc = np.mean((targets_idx == data_idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def csv_print_batch_accuracy(\n",
    "        net, data, targets, train=False, epoch=0, iteration=0, \n",
    "        training_data_file=\"../data/training-testing.csv\"):\n",
    "    spk_rec, _, _ = net(data)\n",
    "    _, data_idx = spk_rec.sum(dim=(1)).max(1)\n",
    "    _, targets_idx = targets.sum(dim=(1)).max(1)\n",
    "    acc = np.mean((targets_idx == data_idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer(net, data, test_data, targets, test_targets):\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(net, data, targets, train=True)\n",
    "    print_batch_accuracy(net, test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architettura e dinamica temporale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "# num_hidden = 1000\n",
    "num_hidden = 512\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 100\n",
    "dt = 1e-2\n",
    "tau_mem = Constant(100e-3)\n",
    "tau_syn = Constant(50e-3)\n",
    "threshold = Constant(1.)\n",
    "bias = Constant(0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Encode del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define a function to encode an input into a poisson event series\n",
    "def encode_poisson(data: torch.Tensor, num_steps: int, scale: float = 0.1) -> torch.Tensor:\n",
    "    num_batches, _, _ = data.shape\n",
    "    data = scale * data.view((num_batches, 1, -1)).repeat((1, num_steps, 1))\n",
    "    return (torch.rand(data.shape) < (data * scale)).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode in spike degli obiettivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define a function to encode the network target\n",
    "def encode_class(class_idx: torch.Tensor, num_classes: int, num_steps: int) -> torch.Tensor:\n",
    "    num_batches = class_idx.numel()\n",
    "    target = torch.nn.functional.one_hot(class_idx, num_classes = num_classes)\n",
    "    return target.view((num_batches, 1, -1)).repeat((1, num_steps, 1)).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "def DefineNet(num_inputs, num_hidden, num_outputs):\n",
    "    return Sequential(\n",
    "        LinearTorch((num_inputs, num_hidden)),\n",
    "        LIFTorch(\n",
    "            num_hidden,\n",
    "            tau_mem=tau_mem,\n",
    "            tau_syn=tau_syn,\n",
    "            threshold=threshold,\n",
    "            bias=bias,\n",
    "            dt=dt\n",
    "        ),\n",
    "        LinearTorch((num_hidden, num_outputs)),\n",
    "        LIFTorch(\n",
    "            num_outputs,\n",
    "            tau_mem=tau_mem,\n",
    "            tau_syn=tau_syn,\n",
    "            threshold=threshold,\n",
    "            bias=bias,\n",
    "            dt=dt\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchSequential  with shape (784, 10) {\n",
       "    LinearTorch '0_LinearTorch' with shape (784, 512)\n",
       "    LIFTorch '1_LIFTorch' with shape (512, 512)\n",
       "    LinearTorch '2_LinearTorch' with shape (512, 10)\n",
       "    LIFTorch '3_LIFTorch' with shape (10, 10)\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Network instantiation\n",
    "net = DefineNet(num_inputs=num_inputs, num_hidden=num_hidden, num_outputs=num_outputs)\n",
    "net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train = False\n",
    "state_dict_file_path = \"../models/Rockpool-MNIST-training.pt\"\n",
    "\n",
    "try:\n",
    "    load_state_dict = torch.load( state_dict_file_path, map_location=device, )\n",
    "    net.load_state_dict(load_state_dict)\n",
    "except FileNotFoundError:\n",
    "    print( \"File not found running training\" )\n",
    "    run_train = True\n",
    "\n",
    "if ( run_train == True ):\n",
    "    num_epochs = 3\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "    # loss = nn.CrossEntropyLoss()\n",
    "    # loss = nn.MSELoss()\n",
    "    loss = torch.nn.functional.mse_loss\n",
    "    optimizer = torch.optim.Adam(net.parameters().astorch())#, lr=5e-4)\n",
    "    net.train()\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # train_batch = iter()\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_loader:\n",
    "            print( \"Epoch \", epoch, \" Iteration: \", counter)\n",
    "            data = encode_poisson(data.squeeze(), num_steps)\n",
    "\n",
    "            targets = encode_class(targets, 10, num_steps)\n",
    "\n",
    "            # - Zero gradients, simulate model\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            spk_rec, _, _ = net(data.to(device))\n",
    "            loss_val = loss(spk_rec, targets.to(device))\n",
    "            print(loss_val)\n",
    "            # Gradient calculation + weight update\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "            counter += 1\n",
    "            print_batch_accuracy(net, data.to(device), targets.to(device), train=True)\n",
    "\n",
    "    torch.save( net.state_dict(), state_dict_file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy for a single minibatch: 90.04%\n",
      "Test set accuracy for a single minibatch: 89.26%\n",
      "Test set accuracy for a single minibatch: 91.60%\n",
      "Test set accuracy for a single minibatch: 89.84%\n",
      "Test set accuracy for a single minibatch: 89.45%\n",
      "Test set accuracy for a single minibatch: 89.65%\n",
      "Test set accuracy for a single minibatch: 89.45%\n",
      "Test set accuracy for a single minibatch: 89.65%\n",
      "Test set accuracy for a single minibatch: 88.09%\n",
      "Test set accuracy for a single minibatch: 89.84%\n",
      "Test set accuracy for a single minibatch: 89.45%\n",
      "Test set accuracy for a single minibatch: 88.87%\n",
      "Test set accuracy for a single minibatch: 90.62%\n",
      "Test set accuracy for a single minibatch: 85.94%\n",
      "Test set accuracy for a single minibatch: 89.45%\n",
      "Test set accuracy for a single minibatch: 88.67%\n",
      "Test set accuracy for a single minibatch: 89.45%\n",
      "Test set accuracy for a single minibatch: 90.82%\n",
      "Test set accuracy for a single minibatch: 88.09%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for test_data, test_targets in test_loader:\n",
    "        test_data = encode_poisson(test_data.squeeze(), num_steps)\n",
    "        test_data = test_data.to(device)\n",
    "        test_targets = encode_class(test_targets, 10, num_steps)\n",
    "        test_targets = test_targets.to(device)\n",
    "        \n",
    "        # Test set forward pass\n",
    "        test_spk, _, _ = net(test_data)\n",
    "\n",
    "        test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "        test_loss = loss(test_spk, test_targets)\n",
    "\n",
    "        test_loss_hist.append(test_loss.item())\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        print_batch_accuracy(net, test_data.to(device), test_targets.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10682813078165054, 0.10914649069309235, 0.11120312660932541, 0.12012305110692978, 0.12407618016004562, 0.12315430492162704, 0.13258399069309235, 0.13133594393730164, 0.1283300817012787, 0.1274140626192093, 0.12962110340595245, 0.12312696129083633, 0.12244727462530136, 0.12012695521116257, 0.11998828500509262, 0.12404688447713852, 0.11854297667741776, 0.11735742539167404, 0.11878320574760437, 0.11824023723602295, 0.1180429756641388, 0.10902539640665054, 0.10792383551597595, 0.10695899277925491, 0.11232031881809235, 0.10446875542402267, 0.10731054842472076, 0.10698828846216202, 0.10663477331399918, 0.10240625590085983, 0.10024414211511612, 0.11371093988418579, 0.10036914795637131, 0.1063535213470459, 0.10422461479902267, 0.10090039670467377, 0.09357226639986038, 0.09573633223772049, 0.10163672268390656, 0.09200391173362732, 0.09450195729732513, 0.09293555468320847, 0.09063086658716202, 0.09485937654972076, 0.0942968800663948, 0.0891738310456276, 0.09135742485523224, 0.0906054750084877, 0.09020117670297623, 0.0892011746764183, 0.09448438137769699, 0.09269727021455765, 0.09421875327825546, 0.09062109887599945, 0.09657032042741776, 0.09029492735862732, 0.09348047524690628, 0.08714257925748825, 0.08691992610692978, 0.08921484649181366, 0.08360937982797623, 0.10477539896965027, 0.08774414658546448, 0.08493360131978989, 0.08405274152755737, 0.0954277366399765, 0.08525390923023224, 0.09085937589406967, 0.09028320759534836, 0.09065821021795273, 0.09139648824930191, 0.0885019600391388, 0.09244727343320847, 0.08703906834125519, 0.08720703423023224, 0.08681250363588333, 0.0843515694141388, 0.08666016161441803, 0.08645117282867432, 0.08655664324760437, 0.08745117485523224, 0.08838086575269699, 0.10540235042572021, 0.08568164706230164, 0.09213476628065109, 0.08817774057388306, 0.08589062839746475, 0.08934375643730164, 0.08615820854902267, 0.08720898628234863, 0.08927734941244125, 0.0928359404206276, 0.08451563119888306, 0.08561524003744125, 0.08494922518730164, 0.0897226631641388, 0.08588282018899918, 0.08638477325439453, 0.08791211247444153, 0.08452735096216202, 0.08308203518390656, 0.08662109822034836, 0.08784180134534836, 0.08648242801427841, 0.0821816474199295, 0.08847852051258087, 0.08465820550918579, 0.08448828756809235, 0.08425390720367432, 0.08513281494379044, 0.08469531685113907, 0.08564063161611557, 0.0937480479478836, 0.08556250482797623, 0.08488867431879044, 0.0823378935456276, 0.09036523848772049, 0.0863320380449295, 0.0856933668255806, 0.08449414372444153, 0.08525000512599945, 0.08618750423192978, 0.08436524122953415, 0.08578711003065109, 0.08576172590255737, 0.08622266352176666, 0.08459179848432541, 0.08542383462190628, 0.08647070825099945, 0.08386524021625519, 0.08587890863418579, 0.08536718785762787, 0.08684961497783661, 0.08692383021116257, 0.08415820449590683, 0.08657226711511612, 0.08450586348772049, 0.09424219280481339, 0.08882617950439453, 0.08510937541723251, 0.09084179997444153, 0.08283399045467377, 0.08150586485862732, 0.08610352128744125, 0.08480078727006912, 0.08847656846046448, 0.08971875160932541, 0.09138672053813934, 0.08502930402755737, 0.08844336122274399, 0.08683203905820847, 0.08952149003744125, 0.08157812803983688, 0.08134765923023224, 0.08485156297683716, 0.08284766227006912, 0.08757422119379044, 0.09091797471046448, 0.08435742557048798, 0.08399414271116257, 0.09065821021795273, 0.09721679985523224, 0.08380664139986038, 0.08595898747444153, 0.0850839912891388, 0.08711914718151093, 0.08556640893220901, 0.08781055361032486, 0.08821094036102295, 0.08684766292572021, 0.08811523765325546, 0.08357813209295273, 0.08335937559604645, 0.08322852104902267, 0.08329297602176666, 0.08725195378065109, 0.08497852087020874, 0.08816211670637131, 0.08668555319309235, 0.08482421934604645, 0.08710547536611557, 0.08539062738418579, 0.07919922471046448, 0.08188086003065109, 0.08582422137260437, 0.08923828601837158, 0.08499804884195328, 0.08556836098432541, 0.08720313012599945, 0.08467578887939453, 0.0905449241399765, 0.08452735096216202, 0.08719336241483688, 0.08394922316074371, 0.08802539110183716, 0.0859980508685112, 0.08732032030820847, 0.08483593910932541, 0.08628320693969727, 0.08363085985183716, 0.08460156619548798, 0.08558789640665054, 0.08349805325269699, 0.08292969316244125, 0.08056055009365082, 0.08179492503404617, 0.08426172286272049, 0.08748438209295273, 0.08634570986032486, 0.08415820449590683, 0.08556640893220901, 0.08688672631978989, 0.08778516203165054, 0.08645117282867432, 0.08283203840255737, 0.08288867771625519, 0.08353711664676666, 0.08150976896286011, 0.08085937798023224, 0.08171875029802322, 0.08316211402416229, 0.09285547584295273, 0.08414258062839508, 0.083921879529953, 0.08473047614097595, 0.08421485126018524, 0.08516992628574371, 0.07993359863758087, 0.08435937762260437, 0.08299023658037186, 0.08136719465255737, 0.08170117437839508, 0.08384180068969727, 0.08394922316074371, 0.08878320455551147, 0.08180469274520874, 0.08902539312839508, 0.08635938167572021, 0.08267968893051147, 0.0806894600391388, 0.08479297161102295, 0.08165820688009262, 0.07416406273841858, 0.08513867855072021, 0.08674609661102295, 0.086119145154953, 0.08000781387090683, 0.08368359506130219, 0.08911719173192978, 0.08533594012260437, 0.08037109673023224, 0.08399219065904617, 0.0842285230755806, 0.08152734488248825, 0.08411718904972076, 0.08382031321525574, 0.08280859887599945, 0.08498047292232513, 0.08438672125339508, 0.08063672482967377, 0.0826406255364418, 0.08553711324930191, 0.08446484804153442, 0.08182422071695328, 0.08366601914167404, 0.08878906816244125, 0.08402930200099945, 0.08153906464576721, 0.08112891018390656, 0.07929101586341858, 0.08395899087190628, 0.07902734726667404, 0.08156640827655792, 0.08343164622783661, 0.08301758021116257, 0.08208008110523224, 0.08656054735183716, 0.08290039747953415, 0.08647656440734863, 0.08861133456230164, 0.08139649033546448, 0.08211132884025574, 0.08418164402246475, 0.08174609392881393, 0.08053125441074371, 0.08083593845367432, 0.0843808650970459, 0.07868359982967377, 0.08110547065734863, 0.07919531315565109, 0.08713086694478989, 0.0842871144413948, 0.08466406911611557, 0.08061523735523224, 0.08451171964406967, 0.07965625077486038, 0.07885351777076721, 0.07954687625169754, 0.07860937714576721, 0.08763672411441803, 0.08353711664676666, 0.0811777412891388, 0.08258593827486038, 0.0813574269413948, 0.07603125274181366, 0.08116602152585983, 0.08244726806879044, 0.08179882913827896, 0.08273633569478989, 0.08545313030481339, 0.08324414491653442, 0.08452930301427841, 0.08026757836341858, 0.0807168036699295, 0.08021289110183716, 0.0818789079785347, 0.08357226848602295, 0.08024805039167404, 0.0824589878320694, 0.07778320461511612, 0.08194726705551147, 0.07712695747613907, 0.08220312744379044, 0.08732032030820847, 0.08133398741483688, 0.08320508152246475, 0.07977539300918579, 0.08128906786441803, 0.08490429818630219, 0.0796210989356041, 0.0814199224114418, 0.07985156774520874, 0.08091016113758087, 0.07935351878404617, 0.08039258420467377, 0.07905664294958115, 0.08011328428983688, 0.08161914348602295, 0.08319531381130219, 0.08754687756299973, 0.0818457081913948, 0.08039844036102295, 0.07896680384874344, 0.07821875065565109, 0.08057422190904617, 0.08379688113927841, 0.08110742270946503, 0.08072461187839508, 0.0820898488163948, 0.0831914097070694, 0.07972852140665054]\n",
      "[0.0821816474199295, 0.08176367729902267, 0.0773320347070694, 0.08192578703165054, 0.08025586605072021, 0.08093750476837158, 0.07921484857797623, 0.08463086187839508, 0.07996094226837158, 0.08054102212190628, 0.08136914670467377, 0.08124218881130219, 0.07800000160932541, 0.07965821027755737, 0.08198828250169754, 0.08445117622613907, 0.08011914789676666, 0.07736914604902267, 0.08463672548532486, 0.08363867551088333, 0.0796191468834877, 0.08435352146625519, 0.0776972696185112, 0.08177539706230164, 0.07815820723772049, 0.0767226591706276, 0.08360937982797623, 0.07991406321525574, 0.0771191418170929, 0.07968750596046448, 0.07850195467472076, 0.07808398455381393, 0.08201172202825546, 0.0792851597070694, 0.077392578125, 0.0818164125084877, 0.083677738904953, 0.07770508527755737]\n"
     ]
    }
   ],
   "source": [
    "print(loss_hist)\n",
    "print(test_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data, targets in test_loader:\n",
    "    data = encode_poisson(data.squeeze(), num_steps)\n",
    "    targets = encode_class(targets, 10, num_steps)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    test_spk, _, _ = net(data.to(device))\n",
    "\n",
    "    # calculate total accuracy\n",
    "    _, predicted = test_spk.sum(1).max(1)\n",
    "    total += len(targets.sum(1).max(1).indices)\n",
    "    correct += (predicted == targets.sum(1).max(1).indices).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8951\n",
      "89.51\n"
     ]
    }
   ],
   "source": [
    "print(total)\n",
    "print(correct)\n",
    "print(\"{:.2f}\".format(100*(correct/total)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
